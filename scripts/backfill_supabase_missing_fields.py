#!/usr/bin/env python3
"""Backfill Supabase tables with fields from local CSV outputs.

The Streamlit app expects a number of columns (close, MA, supports, etc.)
that were generated by the data ingestion scripts and written to
`openbb_outputs/`.  During recent schema iterations, several of those fields
were either dropped or left empty inside Supabase.  This helper reads the
local CSVs and pushes the missing pieces back to Supabase so that the
dashboard (especially the holdings/risk module) can render complete metrics.
"""

from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import numpy as np
import pandas as pd
import requests
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

DATA_DIR = Path(__file__).resolve().parent.parent / "openbb_outputs"
SUMMARY_CSV = DATA_DIR / "three_month_summary.csv"
HISTORY_CSV = DATA_DIR / "three_month_close_history.csv"
SUMMARY_TABLE = os.environ.get("SUPABASE_SUMMARY_TABLE", "equity_metrics")
HISTORY_TABLE = os.environ.get("SUPABASE_HISTORY_TABLE", "equity_metrics_history")
SUPABASE_URL = os.environ.get(
    "SUPABASE_URL", "https://wpyrevceqirzpwcpulqz.supabase.co"
)
SUPABASE_KEY = os.environ.get(
    "SUPABASE_KEY",
    "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6IndweXJldmNlcWlyenB3Y3B1bHF6Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjMzODUzOTEsImV4cCI6MjA3ODk2MTM5MX0.vY-lSpINIwDc80Caq7tX6iQ_zcBaKDflO5AfV79-tZA",
)

HEADERS = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
}
SESSION = requests.Session()
SESSION.verify = False

# Fields we want to backfill for the summary + history tables.  Only the
# columns that already exist in Supabase will be used.
SUMMARY_FIELDS = [
    "support_level_primary",
    "support_level_secondary",
    "pct_change",
    "pct_change_7d",
    "pct_change_30d",
    "value_score",
    "value_score_tier",
    "entry_recommendation",
]
HISTORY_FIELD_MAP = {
    "open": "open",
    "high": "high",
    "low": "low",
    "close": "close",
    "close_norm": "close_norm",
    "close_percentile": "close_percentile",
    "support_level": "support_level_primary",
    "support_level_secondary": "support_level_secondary",
    "ttm_eps": "ttm_eps",
    "pe": "pe",
    "ps_ratio": "ps_ratio",
    "ma50": "ma50",
    "ma200": "ma200",
    "rsi14": "rsi14",
    "fib_38_2": "fib_38_2",
    "fib_50": "fib_50",
    "fib_61_8": "fib_61_8",
    "volume": "volume",
    "volume_ma20": "volume_ma20",
    "volume_spike_ratio": "volume_spike_ratio",
    "obv": "obv",
    "vpt": "vpt",
    "vwap": "vwap",
    "ad_line": "ad_line",
}


def read_csv(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(f"Missing source file: {path}")
    return pd.read_csv(path)


def dataframe_to_records(df: pd.DataFrame, date_cols: Iterable[str] | None = None) -> List[Dict]:
    out = df.copy()
    if date_cols:
        for col in date_cols:
            if col in out.columns:
                out[col] = pd.to_datetime(out[col], errors="coerce").dt.strftime("%Y-%m-%d")
    return out.replace({np.nan: None}).to_dict(orient="records")


def get_table_columns(table: str) -> List[str]:
    resp = SESSION.get(
        f"{SUPABASE_URL}/rest/v1/{table}",
        headers=HEADERS,
        params={"select": "*", "limit": 1},
        timeout=30,
    )
    resp.raise_for_status()
    rows = resp.json()
    if not rows:
        raise RuntimeError(f"Table `{table}` is empty; cannot infer columns.")
    return list(rows[0].keys())


def _delete_existing(table: str, conflict_cols: List[str], record: Dict) -> None:
    filters = {}
    for col in conflict_cols:
        value = record.get(col)
        if value is None:
            return
        filters[col] = f"eq.{value}"
    SESSION.delete(
        f"{SUPABASE_URL}/rest/v1/{table}",
        headers=HEADERS,
        params=filters,
        timeout=30,
    )


def upsert_records(table: str, records: List[Dict], conflict_cols: List[str]) -> int:
    if not records:
        return 0
    insert_headers = {**HEADERS, "Content-Type": "application/json", "Prefer": "return=minimal"}
    total = 0
    for record in records:
        _delete_existing(table, conflict_cols, record)
        resp = SESSION.post(
            f"{SUPABASE_URL}/rest/v1/{table}",
            headers=insert_headers,
            data=json.dumps([record]),
            timeout=30,
        )
        try:
            resp.raise_for_status()
        except requests.HTTPError as exc:  # noqa: BLE001
            raise RuntimeError(
                f"Supabase insert failed for table `{table}`: {resp.text}"
            ) from exc
        total += 1
    return total


def prepare_summary_records(columns: List[str]) -> Tuple[List[Dict], List[str]]:
    summary_df = read_csv(SUMMARY_CSV)
    available_fields = [field for field in SUMMARY_FIELDS if field in columns]
    missing = sorted(set(SUMMARY_FIELDS) - set(available_fields))
    if not available_fields:
        return [], missing
    payload = summary_df[["symbol"] + available_fields].copy()
    records = dataframe_to_records(payload)
    return records, missing


def prepare_history_records(columns: List[str]) -> Tuple[List[Dict], List[str]]:
    history_df = read_csv(HISTORY_CSV)
    usable_pairs = [
        (csv_col, target_col)
        for csv_col, target_col in HISTORY_FIELD_MAP.items()
        if target_col in columns and csv_col in history_df.columns
    ]
    missing_targets = sorted(
        set(HISTORY_FIELD_MAP.values()) - {target for _, target in usable_pairs}
    )
    if not usable_pairs:
        return [], missing_targets
    payload = pd.DataFrame()
    payload["symbol"] = history_df["symbol"]
    payload["as_of_date"] = pd.to_datetime(history_df["date"], errors="coerce").dt.strftime(
        "%Y-%m-%d"
    )
    for csv_col, target_col in usable_pairs:
        payload[target_col] = history_df[csv_col]
    records = dataframe_to_records(payload, date_cols=["as_of_date"])
    return records, missing_targets


def main() -> None:
    if not SUPABASE_URL or not SUPABASE_KEY:
        raise SystemExit("Missing SUPABASE_URL or SUPABASE_KEY environment variables.")

    summary_cols = get_table_columns(SUMMARY_TABLE)
    summary_records, summary_missing = prepare_summary_records(summary_cols)
    summary_updates = upsert_records(SUMMARY_TABLE, summary_records, ["symbol"])

    history_cols = get_table_columns(HISTORY_TABLE)
    history_records, history_missing = prepare_history_records(history_cols)
    history_updates = upsert_records(
        HISTORY_TABLE, history_records, ["symbol", "as_of_date"]
    )

    print(f"[Summary] Upserted {summary_updates} rows into `{SUMMARY_TABLE}`.")
    if summary_missing:
        print(
            f"[Summary] Skipped columns not present in table: {', '.join(summary_missing)}"
        )
    print(f"[History] Upserted {history_updates} rows into `{HISTORY_TABLE}`.")
    if history_missing:
        print(
            f"[History] Skipped columns not present in table: {', '.join(history_missing)}"
        )


if __name__ == "__main__":
    main()
